{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sid040703/Fall-Detection-For-Elderly-People/blob/main/Fall_Detection_For_Elderly_People.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQSFu60DT5Zy",
        "outputId": "7a29248f-fb68-434e-890e-23edb83c8c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sryVyZHvSC3v",
        "outputId": "2d39e93e-48d6-49ef-8d43-94e3ecd3fb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3SLpHbVmHEYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense"
      ],
      "metadata": {
        "id": "Pl_9nMovLlmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_from_video(video_path, output_dir, frame_rate=1):\n",
        "    \"\"\"\n",
        "    Extract frames from a video and save them to the specified directory.\n",
        "    Args:\n",
        "        video_path (str): Path to the input video.\n",
        "        output_dir (str): Directory to save the extracted frames.\n",
        "        frame_rate (int): Number of frames to extract per second.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_interval = max(1, fps // frame_rate)\n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_interval == 0:\n",
        "            frame_filename = os.path.join(output_dir, f\"frame_{saved_count:04d}.jpg\")\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Extracted {saved_count} frames from {video_path}.\")\n",
        "\n",
        "# Extract frames from a sample video\n",
        "video_path = '/content/WhatsApp Video 2024-11-29 at 15.32.47.mp4'\n",
        "frame_dir = '/content/drive/My Drive/Colab Notebooks/dataset/ur_fall/frames/fall'\n",
        "extract_frames_from_video(video_path, frame_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrhJ4I_ELw-B",
        "outputId": "252ca677-fafd-4790-fa2d-539ecf196e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 0 frames from /content/WhatsApp Video 2024-11-29 at 15.32.47.mp4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_frames_and_labels(frame_dir):\n",
        "    \"\"\"\n",
        "    Load paths to frames and their corresponding labels.\n",
        "    Args:\n",
        "        frame_dir (str): Path to the directory containing fall and non-fall frames.\n",
        "    Returns:\n",
        "        list, list: Paths to frames and corresponding labels.\n",
        "    \"\"\"\n",
        "    frame_paths, labels = [], []\n",
        "    for label_dir in os.listdir(frame_dir):\n",
        "        label_path = os.path.join(frame_dir, label_dir)\n",
        "        label = 0 if 'fall' in label_dir.lower() else 1  # 0: Fall, 1: Non-Fall\n",
        "        for frame in os.listdir(label_path):\n",
        "            frame_paths.append(os.path.join(label_path, frame))\n",
        "            labels.append(label)\n",
        "    return frame_paths, labels\n",
        "\n",
        "# Load frame paths and labels\n",
        "frame_dir = '/content/drive/MyDrive/Colab Notebooks/ur_fall/frames'\n",
        "frame_paths, labels = load_frames_and_labels(frame_dir)\n",
        "print(f\"Loaded {len(frame_paths)} frames with labels.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class OpenPoseLight:\n",
        "    def detect(self, frame):\n",
        "        \"\"\"\n",
        "        Simulate pose detection. Replace with OpenPose-light implementation.\n",
        "        Args:\n",
        "            frame: Input image frame.\n",
        "        Returns:\n",
        "            dict: Simulated pose keypoints and aspect ratio.\n",
        "        \"\"\"\n",
        "        keypoints = np.random.rand(18, 2)  # Simulated 18 keypoints (x, y)\n",
        "        aspect_ratio = 1.0  # Simulated aspect ratio\n",
        "        return {'keypoints': keypoints, 'aspect_ratio': aspect_ratio}\n",
        "\n",
        "# Initialize pose detector\n",
        "pose_detector = OpenPoseLight()\n",
        "\n",
        "\n",
        "\n",
        "def generate_sequences_from_frames(frame_paths, labels, pose_detector, sequence_length=16):\n",
        "    \"\"\"\n",
        "    Generate sequences of pose keypoints from frames.\n",
        "    Args:\n",
        "        frame_paths (list): List of frame paths.\n",
        "        labels (list): List of corresponding labels.\n",
        "        pose_detector (object): Pose detection model.\n",
        "        sequence_length (int): Number of frames per sequence.\n",
        "    Returns:\n",
        "        np.array, np.array: Sequences and labels.\n",
        "    \"\"\"\n",
        "    sequences, sequence_labels = [], []\n",
        "    sequence, label_sequence = [], []\n",
        "\n",
        "    for frame_path, label in zip(frame_paths, labels):\n",
        "        frame = cv2.imread(frame_path)\n",
        "        if frame is None:\n",
        "            continue  # Skip invalid frames\n",
        "\n",
        "        keypoints = pose_detector.detect(frame)['keypoints'].flatten()\n",
        "        sequence.append(keypoints)\n",
        "        label_sequence.append(label)\n",
        "\n",
        "        if len(sequence) == sequence_length:\n",
        "            sequences.append(sequence.copy())\n",
        "            sequence_labels.append(label_sequence[-1])\n",
        "            sequence.pop(0)  # Sliding window\n",
        "            label_sequence.pop(0)\n",
        "\n",
        "    return np.array(sequences), np.array(sequence_labels)\n",
        "\n",
        "# Generate sequences\n",
        "sequence_length = 16\n",
        "X_seq, y_seq = generate_sequences_from_frames(frame_paths, labels, pose_detector, sequence_length)\n",
        "print(f\"Generated {len(X_seq)} sequences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxiqvXKLHbds",
        "outputId": "d8c141a7-36de-44da-f6c5-2309ac72dc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2371 frames with labels.\n",
            "Generated 2356 sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "def create_lstm_model():\n",
        "    \"\"\"\n",
        "    Create an LSTM model for fall detection.\n",
        "    Returns:\n",
        "        keras.Model: Compiled LSTM model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=(16, 36), return_sequences=True),  # 16 frames, 36 keypoints\n",
        "        LSTM(64),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(2, activation='softmax')  # Output: Fall or Non-Fall\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "lstm_model = create_lstm_model()\n",
        "lstm_model.fit(X_seq, y_seq, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Save the trained model\n",
        "lstm_model.save('/content/drive/My Drive/Colab Notebooks/dataset/lstm_fall_detection.h5')\n",
        "print(\"Model saved to Google Drive.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sno6Ya3tH3b3",
        "outputId": "89416e2f-a6b5-4073-b4b5-cfed66473f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 50ms/step - accuracy: 0.9927 - loss: 0.1470 - val_accuracy: 1.0000 - val_loss: 2.2175e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 2.0373e-05 - val_accuracy: 1.0000 - val_loss: 1.6827e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 1.5948e-05 - val_accuracy: 1.0000 - val_loss: 1.3392e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 1.2677e-05 - val_accuracy: 1.0000 - val_loss: 1.0690e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 1.0148e-05 - val_accuracy: 1.0000 - val_loss: 8.6394e-06\n",
            "Epoch 6/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 8.2317e-06 - val_accuracy: 1.0000 - val_loss: 7.0952e-06\n",
            "Epoch 7/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 6.7843e-06 - val_accuracy: 1.0000 - val_loss: 5.9178e-06\n",
            "Epoch 8/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 5.6765e-06 - val_accuracy: 1.0000 - val_loss: 5.0055e-06\n",
            "Epoch 9/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 4.8221e-06 - val_accuracy: 1.0000 - val_loss: 4.2913e-06\n",
            "Epoch 10/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 4.1434e-06 - val_accuracy: 1.0000 - val_loss: 3.7235e-06\n",
            "Epoch 11/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 3.6001e-06 - val_accuracy: 1.0000 - val_loss: 3.2606e-06\n",
            "Epoch 12/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 3.1602e-06 - val_accuracy: 1.0000 - val_loss: 2.8762e-06\n",
            "Epoch 13/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.7972e-06 - val_accuracy: 1.0000 - val_loss: 2.5587e-06\n",
            "Epoch 14/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.4888e-06 - val_accuracy: 1.0000 - val_loss: 2.2864e-06\n",
            "Epoch 15/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.0177e-06 - val_accuracy: 1.0000 - val_loss: 4.7684e-07\n",
            "Epoch 16/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 3.0623e-07 - val_accuracy: 1.0000 - val_loss: 1.1921e-07\n",
            "Epoch 17/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.7017e-08 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 18/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 19/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 20/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 21/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 22/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 23/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 24/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 25/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 26/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 27/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 28/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 29/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 30/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 31/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 32/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 33/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 34/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 35/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 36/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 37/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 38/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 39/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 40/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 41/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 42/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 43/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 44/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 45/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 46/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 47/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 48/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 49/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 50/50\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example reshaping\n",
        "!pip install mediapipe\n",
        "\n"
      ],
      "metadata": {
        "id": "TeJnve6LVzy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4ef8f3-94ca-4d1f-eba0-a9c769e8925d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.20 sounddevice-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH-1OueLRV-d",
        "outputId": "5b9da543-6387-4c5c-9b49-828cbe97e410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.20)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from Mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->Mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->Mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->Mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->Mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->Mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->Mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->Mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->Mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->Mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->Mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->Mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->Mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->Mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->Mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from collections import deque\n",
        "from mediapipe.python.solutions.pose import Pose  # Mediapipe for pose detection\n",
        "from google.colab import files  # For downloading files\n",
        "import os\n",
        "\n",
        "def record_video(filename='recorded_video.webm'):\n",
        "    \"\"\"\n",
        "    Records a video from the webcam and saves it locally.\n",
        "    \"\"\"\n",
        "    js = Javascript('''\n",
        "    async function recordVideo() {\n",
        "        const div = document.createElement('div');\n",
        "        const recordButton = document.createElement('button');\n",
        "        const stopButton = document.createElement('button');\n",
        "        const video = document.createElement('video');\n",
        "\n",
        "        recordButton.textContent = 'Start Recording';\n",
        "        stopButton.textContent = 'Stop Recording';\n",
        "        stopButton.style.display = 'none';\n",
        "\n",
        "        div.appendChild(recordButton);\n",
        "        div.appendChild(stopButton);\n",
        "        div.appendChild(video);\n",
        "\n",
        "        document.body.appendChild(div);\n",
        "\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        video.srcObject = stream;\n",
        "        video.style.display = 'block';\n",
        "        await video.play();\n",
        "\n",
        "        const recorder = new MediaRecorder(stream);\n",
        "        const chunks = [];\n",
        "\n",
        "        recorder.ondataavailable = (e) => chunks.push(e.data);\n",
        "        recorder.onstop = () => {\n",
        "            const blob = new Blob(chunks, {type: 'video/webm'});\n",
        "            const reader = new FileReader();\n",
        "            reader.readAsDataURL(blob);\n",
        "            reader.onloadend = () => {\n",
        "                const base64data = reader.result.split(',')[1];\n",
        "                google.colab.kernel.invokeFunction('notebook.save_video', [base64data], {});\n",
        "            };\n",
        "        };\n",
        "\n",
        "        recordButton.onclick = () => {\n",
        "            recorder.start();\n",
        "            recordButton.style.display = 'none';\n",
        "            stopButton.style.display = 'inline-block';\n",
        "        };\n",
        "\n",
        "        stopButton.onclick = () => {\n",
        "            recorder.stop();\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            div.remove();\n",
        "        };\n",
        "\n",
        "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    eval_js('recordVideo()')\n",
        "\n",
        "\n",
        "def save_video(data, filename='recorded_video.webm'):\n",
        "    \"\"\"\n",
        "    Save base64-encoded video data to a file.\n",
        "    \"\"\"\n",
        "    binary = b64decode(data)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    print(f\"Video saved as {filename}\")\n",
        "\n",
        "\n",
        "def process_recorded_video(video_path, lstm_model_path, output_path='/content/processed_video.avi'):\n",
        "    \"\"\"\n",
        "    Processes the recorded video and performs fall detection, saving the output video.\n",
        "    \"\"\"\n",
        "    # Load LSTM model\n",
        "    lstm_model = load_model(lstm_model_path)\n",
        "\n",
        "    # Buffer for storing features for LSTM input\n",
        "    feature_buffer = deque(maxlen=16)\n",
        "\n",
        "    # Initialize Mediapipe Pose\n",
        "    pose_detector = Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5)\n",
        "\n",
        "    # Open the recorded video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video properties\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Create VideoWriter to save processed video\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Process frame with Mediapipe Pose\n",
        "        results = pose_detector.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "            h, w, _ = frame.shape\n",
        "            keypoints = np.array([[lm.x * w, lm.y * h, lm.visibility] for lm in landmarks])\n",
        "\n",
        "            # Normalize keypoints and append to feature buffer\n",
        "            normalized_keypoints = keypoints / np.array([w, h, 1])  # Normalize by frame size\n",
        "            flattened_keypoints = normalized_keypoints.flatten()[:36]  # Truncate to 36 features\n",
        "            feature_buffer.append(flattened_keypoints)\n",
        "\n",
        "            # Predict using LSTM if buffer is full\n",
        "            label = \"Not Falling\"\n",
        "            color = (0, 255, 0)  # Green for not falling\n",
        "            if len(feature_buffer) == feature_buffer.maxlen:\n",
        "                input_data = np.expand_dims(np.array(feature_buffer), axis=0)\n",
        "                prediction = lstm_model.predict(input_data)\n",
        "                if prediction[0][0] > 0.5:\n",
        "                    label = \"Falling\"\n",
        "                    color = (0, 0, 255)  # Red for falling\n",
        "\n",
        "            # Draw the label on the frame\n",
        "            cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    pose_detector.close()\n",
        "    print(f\"Processed video saved as {output_path}\")\n",
        "\n",
        "\n",
        "# Define a handler to save the recorded video and pass it to the processing function\n",
        "def handle_video(data):\n",
        "    video_filename = \"/content/recorded_video.webm\"\n",
        "    processed_video_filename = \"/content/processed_video.avi\"\n",
        "    save_video(data, video_filename)\n",
        "\n",
        "    # Process the video using the fall detection model\n",
        "    lstm_model_path = \"/content/drive/My Drive/Colab Notebooks/dataset/lstm_fall_detection.h5\"\n",
        "    process_recorded_video(video_filename, lstm_model_path, processed_video_filename)\n",
        "\n",
        "    # List files to ensure processed video exists\n",
        "    print(\"Files in Colab storage:\", os.listdir('/content/'))\n",
        "\n",
        "    # Download the processed video to local PC\n",
        "    files.download(processed_video_filename)\n",
        "\n",
        "\n",
        "# Bind the save function to the notebook\n",
        "from google.colab import output\n",
        "output.register_callback('notebook.save_video', handle_video)\n",
        "\n",
        "# Workflow\n",
        "try:\n",
        "    # Step 1: Record the video\n",
        "    record_video()\n",
        "\n",
        "except Exception as err:\n",
        "    print(f\"An error occurred: {err}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ncXkv7TmjCI7",
        "outputId": "6d943a38-e709-4499-8b55-c47a8d963c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function recordVideo() {\n",
              "        const div = document.createElement('div');\n",
              "        const recordButton = document.createElement('button');\n",
              "        const stopButton = document.createElement('button');\n",
              "        const video = document.createElement('video');\n",
              "\n",
              "        recordButton.textContent = 'Start Recording';\n",
              "        stopButton.textContent = 'Stop Recording';\n",
              "        stopButton.style.display = 'none';\n",
              "\n",
              "        div.appendChild(recordButton);\n",
              "        div.appendChild(stopButton);\n",
              "        div.appendChild(video);\n",
              "\n",
              "        document.body.appendChild(div);\n",
              "\n",
              "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "        video.srcObject = stream;\n",
              "        video.style.display = 'block';\n",
              "        await video.play();\n",
              "\n",
              "        const recorder = new MediaRecorder(stream);\n",
              "        const chunks = [];\n",
              "\n",
              "        recorder.ondataavailable = (e) => chunks.push(e.data);\n",
              "        recorder.onstop = () => {\n",
              "            const blob = new Blob(chunks, {type: 'video/webm'});\n",
              "            const reader = new FileReader();\n",
              "            reader.readAsDataURL(blob);\n",
              "            reader.onloadend = () => {\n",
              "                const base64data = reader.result.split(',')[1];\n",
              "                google.colab.kernel.invokeFunction('notebook.save_video', [base64data], {});\n",
              "            };\n",
              "        };\n",
              "\n",
              "        recordButton.onclick = () => {\n",
              "            recorder.start();\n",
              "            recordButton.style.display = 'none';\n",
              "            stopButton.style.display = 'inline-block';\n",
              "        };\n",
              "\n",
              "        stopButton.onclick = () => {\n",
              "            recorder.stop();\n",
              "            stream.getTracks().forEach(track => track.stop());\n",
              "            div.remove();\n",
              "        };\n",
              "\n",
              "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved as /content/recorded_video.webm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from collections import deque\n",
        "from mediapipe.python.solutions.pose import Pose  # Use Mediapipe for pose detection\n",
        "\n",
        "def calculate_bounding_box(keypoints):\n",
        "    \"\"\"Calculate bounding box around the person based on keypoints.\"\"\"\n",
        "    valid_keypoints = keypoints[keypoints[:, 2] > 0.5]  # Filter keypoints with confidence > 0.5\n",
        "    if valid_keypoints.size == 0:\n",
        "        return None  # No valid keypoints\n",
        "    x_coords = valid_keypoints[:, 0]\n",
        "    y_coords = valid_keypoints[:, 1]\n",
        "    min_x, max_x = int(np.min(x_coords)), int(np.max(x_coords))\n",
        "    min_y, max_y = int(np.min(y_coords)), int(np.max(y_coords))\n",
        "    return (min_x, min_y, max_x, max_y)\n",
        "\n",
        "def is_falling(keypoints):\n",
        "    \"\"\"\n",
        "    Determine if the person is falling based on keypoints.\n",
        "    Example logic: head (keypoint 0) is below hips (keypoints 11, 12).\n",
        "    \"\"\"\n",
        "    if keypoints[0, 2] > 0.5 and keypoints[11, 2] > 0.5 and keypoints[12, 2] > 0.5:\n",
        "        head_y = keypoints[0, 1]  # Y-coordinate of head\n",
        "        hip_y = np.mean([keypoints[11, 1], keypoints[12, 1]])  # Average Y-coordinate of hips\n",
        "        return head_y > hip_y  # Head below hips indicates falling\n",
        "    return False\n",
        "\n",
        "def process_video(video_path, output_path, lstm_model_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "    # Load LSTM model\n",
        "    lstm_model = load_model(lstm_model_path)\n",
        "\n",
        "    # Buffer for storing features for LSTM input\n",
        "    feature_buffer = deque(maxlen=16)\n",
        "\n",
        "    # Initialize Mediapipe Pose\n",
        "    pose_detector = Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Process frame with Mediapipe Pose\n",
        "        results = pose_detector.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "            h, w, _ = frame.shape\n",
        "            keypoints = np.array([[lm.x * w, lm.y * h, lm.visibility] for lm in landmarks])\n",
        "\n",
        "            # Normalize keypoints and append to feature buffer\n",
        "            normalized_keypoints = keypoints / np.array([w, h, 1])  # Normalize by frame size\n",
        "            flattened_keypoints = normalized_keypoints.flatten()[:36]  # Truncate to 36 features\n",
        "            feature_buffer.append(flattened_keypoints)\n",
        "\n",
        "            # Predict using LSTM if buffer is full\n",
        "            label = \"Not Falling\"\n",
        "            color = (0, 255, 0)  # Green for not falling\n",
        "            if len(feature_buffer) == feature_buffer.maxlen:\n",
        "                input_data = np.expand_dims(np.array(feature_buffer), axis=0)\n",
        "                prediction = lstm_model.predict(input_data)\n",
        "                if prediction[0][0] > 0.5:\n",
        "                    if is_falling(keypoints):  # Confirm fall based on pose\n",
        "                        label = \"Falling\"\n",
        "                        color = (0, 0, 255)  # Red for falling\n",
        "\n",
        "            # Calculate bounding box around the person\n",
        "            bbox = calculate_bounding_box(keypoints)\n",
        "            if bbox:\n",
        "                min_x, min_y, max_x, max_y = bbox\n",
        "\n",
        "                # Draw bounding box and label\n",
        "                cv2.rectangle(frame, (min_x, min_y), (max_x, max_y), color, 2)\n",
        "                cv2.rectangle(frame, (min_x, min_y - 30), (max_x, min_y), color, -1)  # Background for text\n",
        "                cv2.putText(frame, label, (min_x + 10, min_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "\n",
        "        # Write frame to output video\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    pose_detector.close()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Example usage\n",
        "process_video(\n",
        "    video_path=\"/content/video.mp4\",\n",
        "    output_path=\"output_video.avi\",\n",
        "    lstm_model_path=\"/content/drive/My Drive/Colab Notebooks/dataset/lstm_fall_detection.h5\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "nH2t3fnfTGm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import os\n",
        "\n",
        "def record_video(filename='video.webm'):\n",
        "    js = Javascript('''\n",
        "    async function recordVideo() {\n",
        "        const div = document.createElement('div');\n",
        "        const recordButton = document.createElement('button');\n",
        "        const stopButton = document.createElement('button');\n",
        "        const video = document.createElement('video');\n",
        "\n",
        "        recordButton.textContent = 'Start Recording';\n",
        "        stopButton.textContent = 'Stop Recording';\n",
        "        stopButton.style.display = 'none';\n",
        "\n",
        "        div.appendChild(recordButton);\n",
        "        div.appendChild(stopButton);\n",
        "        div.appendChild(video);\n",
        "\n",
        "        document.body.appendChild(div);\n",
        "\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        video.srcObject = stream;\n",
        "        video.style.display = 'block';\n",
        "        await video.play();\n",
        "\n",
        "        const recorder = new MediaRecorder(stream);\n",
        "        const chunks = [];\n",
        "\n",
        "        recorder.ondataavailable = (e) => chunks.push(e.data);\n",
        "        recorder.onstop = () => {\n",
        "            const blob = new Blob(chunks, {type: 'video/webm'});\n",
        "            const reader = new FileReader();\n",
        "            reader.readAsDataURL(blob);\n",
        "            reader.onloadend = () => {\n",
        "                const base64data = reader.result.split(',')[1];\n",
        "                google.colab.kernel.invokeFunction('notebook.save_video', [base64data], {});\n",
        "            };\n",
        "        };\n",
        "\n",
        "        recordButton.onclick = () => {\n",
        "            recorder.start();\n",
        "            recordButton.style.display = 'none';\n",
        "            stopButton.style.display = 'inline-block';\n",
        "        };\n",
        "\n",
        "        stopButton.onclick = () => {\n",
        "            recorder.stop();\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            div.remove();\n",
        "        };\n",
        "\n",
        "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    eval_js('recordVideo()')\n",
        "\n",
        "def save_video(data, filename='video.webm'):\n",
        "    \"\"\"\n",
        "    Save base64-encoded video data to a file.\n",
        "    \"\"\"\n",
        "    binary = b64decode(data)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    print(f\"Video saved as {filename}\")\n",
        "\n",
        "def convert_to_mp4(input_path='video.webm', output_path='video.mp4'):\n",
        "    \"\"\"\n",
        "    Convert a webm video to mp4 using ffmpeg.\n",
        "    \"\"\"\n",
        "    command = f\"ffmpeg -i {input_path} -c:v libx264 -preset fast -crf 22 -c:a aac -strict experimental {output_path}\"\n",
        "    os.system(command)\n",
        "    print(f\"Video converted to {output_path}\")\n",
        "\n",
        "# Define a handler to save the recorded video and convert it to mp4\n",
        "def handle_video(data):\n",
        "    webm_filename = \"video.webm\"\n",
        "    mp4_filename = \"video.mp4\"\n",
        "\n",
        "    # Save the video\n",
        "    save_video(data, webm_filename)\n",
        "\n",
        "    # Convert to mp4 format\n",
        "    convert_to_mp4(webm_filename, mp4_filename)\n",
        "\n",
        "    # List files for confirmation\n",
        "    print(\"Files in Colab storage:\", os.listdir('/content'))\n",
        "\n",
        "# Bind the save function to the notebook\n",
        "from google.colab import output\n",
        "output.register_callback('notebook.save_video', handle_video)\n",
        "\n",
        "# Workflow\n",
        "try:\n",
        "    record_video()\n",
        "except Exception as err:\n",
        "    print(str(err))\n"
      ],
      "metadata": {
        "id": "VqSpohAjraqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "37b651e1-e9b0-4620-ae1e-f7962078dd0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function recordVideo() {\n",
              "        const div = document.createElement('div');\n",
              "        const recordButton = document.createElement('button');\n",
              "        const stopButton = document.createElement('button');\n",
              "        const video = document.createElement('video');\n",
              "\n",
              "        recordButton.textContent = 'Start Recording';\n",
              "        stopButton.textContent = 'Stop Recording';\n",
              "        stopButton.style.display = 'none';\n",
              "\n",
              "        div.appendChild(recordButton);\n",
              "        div.appendChild(stopButton);\n",
              "        div.appendChild(video);\n",
              "\n",
              "        document.body.appendChild(div);\n",
              "\n",
              "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "        video.srcObject = stream;\n",
              "        video.style.display = 'block';\n",
              "        await video.play();\n",
              "\n",
              "        const recorder = new MediaRecorder(stream);\n",
              "        const chunks = [];\n",
              "\n",
              "        recorder.ondataavailable = (e) => chunks.push(e.data);\n",
              "        recorder.onstop = () => {\n",
              "            const blob = new Blob(chunks, {type: 'video/webm'});\n",
              "            const reader = new FileReader();\n",
              "            reader.readAsDataURL(blob);\n",
              "            reader.onloadend = () => {\n",
              "                const base64data = reader.result.split(',')[1];\n",
              "                google.colab.kernel.invokeFunction('notebook.save_video', [base64data], {});\n",
              "            };\n",
              "        };\n",
              "\n",
              "        recordButton.onclick = () => {\n",
              "            recorder.start();\n",
              "            recordButton.style.display = 'none';\n",
              "            stopButton.style.display = 'inline-block';\n",
              "        };\n",
              "\n",
              "        stopButton.onclick = () => {\n",
              "            recorder.stop();\n",
              "            stream.getTracks().forEach(track => track.stop());\n",
              "            div.remove();\n",
              "        };\n",
              "\n",
              "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved as video.webm\n",
            "Video converted to video.mp4\n",
            "Files in Colab storage: ['.config', 'video.mp4', 'drive', 'video.webm', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on test data.\n",
        "    Args:\n",
        "        model: Trained model.\n",
        "        X_test: Test features.\n",
        "        y_test: Test labels.\n",
        "    \"\"\"\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Split data for testing (example using 80-20 split)\n",
        "split_idx = int(0.8 * len(X_seq))\n",
        "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(lstm_model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BmVz550HeLV",
        "outputId": "db705db5-004c-4d15-d987-b589d059f852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n",
            "Test Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}